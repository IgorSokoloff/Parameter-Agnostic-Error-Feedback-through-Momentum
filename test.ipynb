{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pwd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import scipy\n",
                "import numpy as np\n",
                "from sklearn import datasets\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "import copy\n",
                "from scipy.linalg import qr\n",
                "from sklearn.random_projection import GaussianRandomProjection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"CUDA available?\", torch.cuda.is_available())\n",
                "for i in range(torch.cuda.device_count()):\n",
                "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i in range(torch.cuda.device_count()):\n",
                "    print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "find rac_lora_mnist/outs -type f -name \"*.out\" | while read -r file; do\n",
                "  echo -e \"\\n----------------------------\"\n",
                "  echo \"File: $(basename \"$file\")\"\n",
                "  echo \"\"\n",
                "  cat \"$file\"\n",
                "  echo \"\"\n",
                "done\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pwd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "----------------------------\n",
                        "File: gen_sgd.py\n",
                        "\n",
                        "import torch\n",
                        "from torch.optim.optimizer import Optimizer\n",
                        "\n",
                        "\n",
                        "class SGDGen(Optimizer):\n",
                        "    r\"\"\"\n",
                        "        based on torch.optim.SGD implementation\n",
                        "    \"\"\"\n",
                        "\n",
                        "    def __init__(self, params, lr, eta, n_workers, momentum=0, dampening=0,\n",
                        "                 weight_decay=0, nesterov=False, comp=None, master_comp=None,\n",
                        "                 error_feedback=False):\n",
                        "        if lr < 0.0:\n",
                        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
                        "        if eta is not None:\n",
                        "            if eta < 0.0: \n",
                        "                raise ValueError(\"Invalid eta: {}\".format(eta))\n",
                        "        \n",
                        "        if momentum < 0.0:\n",
                        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
                        "        if weight_decay < 0.0:\n",
                        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
                        "\n",
                        "        defaults = dict(lr=lr, eta=eta, momentum=momentum, dampening=dampening,\n",
                        "                        weight_decay=weight_decay, nesterov=nesterov)\n",
                        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
                        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
                        "        super(SGDGen, self).__init__(params, defaults)\n",
                        "\n",
                        "        self.comp = comp\n",
                        "        self.error_feedback = error_feedback\n",
                        "        if self.error_feedback and self.comp is None:\n",
                        "            raise ValueError(\"For Error-Feedback, compression can't be None\")\n",
                        "\n",
                        "        self.master_comp = master_comp  # should be unbiased, Error-Feedback is not supported at the moment\n",
                        "\n",
                        "        self.n_workers = n_workers\n",
                        "        self.grads_received = 0\n",
                        "\n",
                        "    def __setstate__(self, state):\n",
                        "        super(SGDGen, self).__setstate__(state)\n",
                        "        for group in self.param_groups:\n",
                        "            group.setdefault('nesterov', False)\n",
                        "\n",
                        "    @torch.no_grad()\n",
                        "    def step_local_global(self, w_id, hvp_dict=None, mvr2b_gprev=None):\n",
                        "        \"\"\"Performs a single optimization step.\n",
                        "        Arguments:\n",
                        "            w_id: integer, id of the worker\n",
                        "            closure (callable, optional): A closure that reevaluates the model\n",
                        "                and returns the loss.\n",
                        "        \"\"\"\n",
                        "        loss = None\n",
                        "\n",
                        "        self.grads_received += 1\n",
                        "\n",
                        "        for group in self.param_groups:\n",
                        "            weight_decay = group['weight_decay']\n",
                        "            momentum = group['momentum']\n",
                        "            dampening = group['dampening']\n",
                        "            nesterov = group['nesterov']\n",
                        "\n",
                        "            for p in group['params']:\n",
                        "                if p.grad is None:\n",
                        "                    continue\n",
                        "\n",
                        "                param_state = self.state[p]\n",
                        "\n",
                        "                d_p = p.grad.data\n",
                        "\n",
                        "                if self.error_feedback == \"EF\":\n",
                        "                    error_key = 'error_' + str(w_id)\n",
                        "                    if error_key not in param_state:\n",
                        "                        #loc_grad = d_p.mul(group['lr'])\n",
                        "                        loc_grad = torch.clone(d_p).detach()\n",
                        "                    else:\n",
                        "                        loc_grad = torch.clone(d_p).detach() + param_state[error_key].mul(group['eta'])\n",
                        "                        #loc_grad = d_p.mul(group['lr']) + param_state[error_key]\n",
                        "\n",
                        "                    d_p = self.comp(loc_grad)\n",
                        "                    param_state[error_key] = loc_grad - d_p\n",
                        "                    \n",
                        "                elif self.error_feedback == \"ECONTROL\":\n",
                        "                    error_key = 'error_' + str(w_id)\n",
                        "                    if error_key not in param_state:\n",
                        "                        param_state[error_key] = torch.zeros_like(d_p)\n",
                        "                        \n",
                        "                    h_name = 'h_' + str(w_id)\n",
                        "                    if h_name not in param_state:\n",
                        "                        param_state[h_name] = d_p\n",
                        "                        \n",
                        "                    Delta = self.comp(param_state[error_key].mul(group['eta']) + d_p - param_state[h_name])\n",
                        "                    \n",
                        "                    param_state[error_key] = d_p + param_state[error_key] - param_state[h_name] - Delta\n",
                        "                    param_state[h_name] = param_state[h_name] + Delta\n",
                        "       \n",
                        "                    d_p = param_state[h_name]\n",
                        "                \n",
                        "                elif self.error_feedback == \"EF21\":\n",
                        "                    error_key = 'error_' + str(w_id)\n",
                        "                    if error_key not in param_state:\n",
                        "                        #g0 = self.comp(d_p)                   # <- compress once\n",
                        "                        g0 = d_p                   # <- uncompressed init\n",
                        "                        param_state[error_key] = g0          # store compressed g_i^0\n",
                        "                        d_p = g0                              # send compressed\n",
                        "                    else:\n",
                        "                        param_state[error_key] += self.comp(d_p - param_state[error_key])\n",
                        "                        d_p = param_state[error_key]\n",
                        "                \n",
                        "                elif self.error_feedback == \"EF21_SGDM\":\n",
                        "                    g_key = 'g_' + str(w_id)\n",
                        "                    v_key = 'v_' + str(w_id)\n",
                        "                    if g_key not in param_state or v_key not in param_state:\n",
                        "                        #g0 = self.comp(d_p)\n",
                        "                        g0 = d_p            # <- uncompressed init\n",
                        "                        param_state[g_key] = g0              # g_i^{-1}  = C(∇f_i)\n",
                        "                        param_state[v_key] = g0              # v_i^{-1}  = C(∇f_i)\n",
                        "                        d_p = g0\n",
                        "                    else:\n",
                        "                        param_state[v_key] = d_p.mul(group['eta']) + param_state[v_key].mul(1-group['eta'])\n",
                        "                        param_state[g_key] += self.comp(param_state[v_key] - param_state[g_key])\n",
                        "                    \n",
                        "                        d_p = param_state[g_key]\n",
                        "                \n",
                        "                elif self.error_feedback == \"EF21_SGDM_NORM\":\n",
                        "                    g_key = 'g_' + str(w_id)\n",
                        "                    v_key = 'v_' + str(w_id)\n",
                        "                    # --- initialise states on first call\n",
                        "                    if g_key not in param_state or v_key not in param_state:\n",
                        "                        #g0 = self.comp(d_p)\n",
                        "                        g0 = d_p                               # <- uncompressed init\n",
                        "                        param_state[g_key] = g0              # g_i^{-1}\n",
                        "                        param_state[v_key] = g0              # v_i^{-1}\n",
                        "                        d_p = g0\n",
                        "                    else:\n",
                        "                        # v_iᵏ  ←  (1-ηₖ)v_i^{k-1} + ηₖ ∇f_i\n",
                        "                        param_state[v_key] = d_p.mul(group['eta'])+ param_state[v_key].mul(1 - group['eta'])\n",
                        "                        # g_iᵏ  ←  g_i^{k-1} + C(v_iᵏ – g_i^{k-1})\n",
                        "                        param_state[g_key] += self.comp(param_state[v_key] - param_state[g_key])\n",
                        "                        d_p = param_state[g_key]   # send g_iᵏ       (will be summed later)\n",
                        "                \n",
                        "                elif self.error_feedback == \"EF21_HM_NORM\":\n",
                        "                    g_key, v_key = f\"g_{w_id}\", f\"v_{w_id}\"\n",
                        "                    prev_x       = f\"x_prev_{w_id}\"\n",
                        "                    eta, comp    = group[\"eta\"], self.comp\n",
                        "                    # ---------- first visit ----------------------------------------\n",
                        "                    if g_key not in param_state:\n",
                        "                        param_state[g_key]  = d_p.detach().clone()\n",
                        "                        param_state[v_key]  = d_p.detach().clone()\n",
                        "                        param_state[prev_x] = p.data.detach().clone()\n",
                        "                        d_p = param_state[g_key]                      # message\n",
                        "                    # ---------- main iteration -------------------------------------\n",
                        "                    else:\n",
                        "                        if eta == 1:\n",
                        "                            # exact EF21 path – drop the HVP term entirely\n",
                        "                            param_state[v_key] = d_p.detach().clone()          # <-  clone!\n",
                        "                        else:\n",
                        "                            # • g_curr already in d_p\n",
                        "                            # • hvp_term supplied once from the loop\n",
                        "                            hvp_term  = hvp_dict[p]\n",
                        "                            v_prev = param_state[v_key]\n",
                        "                            param_state[v_key] = ((1 - eta) * (v_prev + hvp_term) + eta * d_p).detach()# good practice\n",
                        "                        \n",
                        "                        param_state[g_key] += comp(param_state[v_key] - param_state[g_key])\n",
                        "                        param_state[prev_x] = p.data.detach().clone()\n",
                        "                        d_p = param_state[g_key]                      # message\n",
                        "                \n",
                        "                # NOTE: new methods goes below\n",
                        "                elif self.error_feedback == \"EF21_RHM_NORM\":\n",
                        "                    g_key, v_key = f\"g_{w_id}\", f\"v_{w_id}\"\n",
                        "                    prev_x       = f\"x_prev_{w_id}\"\n",
                        "                    eta, comp    = group[\"eta\"], self.comp\n",
                        "\n",
                        "                    if g_key not in param_state or v_key not in param_state:\n",
                        "                        g0 = d_p.detach().clone()\n",
                        "                        param_state[g_key]  = g0\n",
                        "                        param_state[v_key]  = g0\n",
                        "                        # initialize snapshot\n",
                        "                        param_state[prev_x] = p.data.detach().clone()\n",
                        "                        d_p = g0\n",
                        "                    else:\n",
                        "                        # d_p is ∇f_i(x^k) from the first backward\n",
                        "                        hvp_term = hvp_dict[p]  # ∇²f_i(x_hat)·(x^k − x^{k−1})\n",
                        "                        param_state[v_key] = (1 - eta) * (param_state[v_key] + hvp_term) + eta * d_p\n",
                        "                        param_state[g_key] += comp(param_state[v_key] - param_state[g_key])\n",
                        "                        d_p = param_state[g_key]\n",
                        "                        # roll snapshot to current x^k (before master update)\n",
                        "                        param_state[prev_x] = p.data.detach().clone()\n",
                        "\n",
                        "                \n",
                        "                \n",
                        "                elif self.error_feedback == \"EF21_IGT_NORM\":\n",
                        "                    # States (per worker)\n",
                        "                    g_key, v_key = f\"g_{w_id}\", f\"v_{w_id}\"\n",
                        "                    prev_x       = f\"x_prev_{w_id}\"\n",
                        "                    eta, comp    = group[\"eta\"], self.comp\n",
                        "\n",
                        "                    if g_key not in param_state or v_key not in param_state:\n",
                        "                        # First call: initialize with current (uncompressed) gradient\n",
                        "                        g0 = d_p.detach().clone()\n",
                        "                        param_state[g_key] = g0        # g_i^{-1}\n",
                        "                        param_state[v_key] = g0        # v_i^{-1}\n",
                        "                        param_state[prev_x] = p.data.detach().clone()   # <-- initialize snapshot\n",
                        "                        d_p = g0\n",
                        "                    else:\n",
                        "                        # d_p is ∇f_i evaluated at the **extrapolated point** (computed in train.py)\n",
                        "                        # v_i^{t+1} = (1-η_t) v_i^t + η_t ∇f_i(x_extr)\n",
                        "                        param_state[v_key] = (1 - eta) * param_state[v_key] + eta * d_p\n",
                        "                        # g_i^{t+1} = g_i^t + C(v_i^{t+1} - g_i^t)\n",
                        "                        param_state[g_key] += comp(param_state[v_key] - param_state[g_key])\n",
                        "                        d_p = param_state[g_key]\n",
                        "                        \n",
                        "                        # keep the rolling \"previous x\" snapshot up to date\n",
                        "                        param_state[prev_x] = p.data.detach().clone()   # <-- advance snapshot\n",
                        "\n",
                        "                elif self.error_feedback == \"EF21_MVR_1b\":\n",
                        "                    g_key, v_key = f\"g_{w_id}\", f\"v_{w_id}\"\n",
                        "                    prevg_key    = f\"prev_grad_{w_id}\"\n",
                        "                    eta, comp    = group[\"eta\"], self.comp\n",
                        "\n",
                        "                    if g_key not in param_state or v_key not in param_state or prevg_key not in param_state:\n",
                        "                        g0 = d_p.detach().clone()      # current grad ~ ∇f_i(x^{t+1})\n",
                        "                        param_state[g_key]   = g0\n",
                        "                        param_state[v_key]   = g0\n",
                        "                        param_state[prevg_key] = g0    # bootstrap prev grad\n",
                        "                        d_p = g0\n",
                        "                    else:\n",
                        "                        g_new  = d_p.detach().clone()                # ∇f_i(x^{t+1})\n",
                        "                        g_old  = param_state[prevg_key]              # (cached) ≈ ∇f_i(x^t)\n",
                        "                        v_prev = param_state[v_key]\n",
                        "                        # v^{t+1} = (1-η_t)(v^t + g_new - g_old) + η_t g_new\n",
                        "                        v_next = (1 - eta) * (v_prev + (g_new - g_old)) + eta * g_new\n",
                        "                        param_state[v_key] = v_next\n",
                        "                        param_state[g_key] += comp(v_next - param_state[g_key])\n",
                        "                        param_state[prevg_key] = g_new               # cache for next iter\n",
                        "                        d_p = param_state[g_key]\n",
                        "\n",
                        "                elif self.error_feedback == \"EF21_MVR_NORM\":\n",
                        "                    # Two-backprop, same minibatch\n",
                        "                    # d_p carries g_new = ∇f_i(x^k; ξ) from loss.backward()\n",
                        "                    # mvr2b_gprev[p] carries g_old = ∇f_i(x^{k-1}; ξ) computed via autograd.grad\n",
                        "                    g_key, v_key = f\"g_{w_id}\", f\"v_{w_id}\"\n",
                        "                    prev_x       = f\"x_prev_{w_id}\"\n",
                        "                    eta, comp    = group[\"eta\"], self.comp\n",
                        "\n",
                        "                    if g_key not in param_state or v_key not in param_state or mvr2b_gprev is None:\n",
                        "                        # Bootstrap (first step): fall back to using current grad only\n",
                        "                        g0 = d_p.detach().clone()\n",
                        "                        param_state[g_key]  = g0\n",
                        "                        param_state[v_key]  = g0\n",
                        "                        param_state[prev_x] = p.data.detach().clone()\n",
                        "                        d_p = g0\n",
                        "                    else:\n",
                        "                        g_new  = d_p.detach().clone()\n",
                        "                        g_old  = mvr2b_gprev[p].detach()\n",
                        "                        v_prev = param_state[v_key]\n",
                        "\n",
                        "                        # v^{t+1} = (1-η)(v^t + g_new - g_old) + η g_new\n",
                        "                        v_next = (1 - eta) * (v_prev + (g_new - g_old)) + eta * g_new\n",
                        "                        param_state[v_key] = v_next\n",
                        "\n",
                        "                        # EF21 aggregation state update: g ← g + C(v_next − g)\n",
                        "                        param_state[g_key] += comp(v_next - param_state[g_key])\n",
                        "                        d_p = param_state[g_key]\n",
                        "\n",
                        "                        # roll snapshot of x to \"current\" for next step's previous\n",
                        "                        param_state[prev_x] = p.data.detach().clone()\n",
                        "\n",
                        "                else:\n",
                        "                    raise ValueError(\"Unknown error feedback type: {}\".format(self.error_feedback))\n",
                        "\n",
                        "                if 'full_grad' not in param_state or self.grads_received == 1:\n",
                        "                    param_state['full_grad'] = torch.clone(d_p).detach()\n",
                        "                else:\n",
                        "                    param_state['full_grad'] += torch.clone(d_p).detach()\n",
                        "\n",
                        "                if self.grads_received == self.n_workers:\n",
                        "                    grad = param_state['full_grad'] / self.n_workers\n",
                        "                    \n",
                        "\n",
                        "                    if self.error_feedback in (\"EF21_SGDM_NORM\", \"EF21_HM_NORM\", \"EF21_IGT_NORM\", \"EF21_RHM_NORM\", \"EF21_MVR_1b\", \"EF21_MVR_NORM\"):\n",
                        "                        norm = grad.norm()\n",
                        "                        if norm > 0:\n",
                        "                            grad = grad / norm\n",
                        "                    # ----------------------------------------------------------\n",
                        "\n",
                        "                    if self.master_comp is not None:\n",
                        "                        grad = self.master_comp(grad)\n",
                        "\n",
                        "                    if weight_decay != 0:\n",
                        "                        grad.add(p, alpha=weight_decay)\n",
                        "                    if momentum != 0:\n",
                        "                        if 'momentum_buffer' not in param_state:\n",
                        "                            buf = param_state['momentum_buffer'] = torch.clone(grad).detach()\n",
                        "                        else:\n",
                        "                            buf = param_state['momentum_buffer']\n",
                        "                            buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n",
                        "                        if nesterov:\n",
                        "                            grad = grad.add(buf, alpha=momentum)\n",
                        "                        else:\n",
                        "                            grad = buf\n",
                        "\n",
                        "                    #with torch.no_grad():\n",
                        "                    p.data.add_(-group['lr'], grad)\n",
                        "                    \n",
                        "\n",
                        "        if self.grads_received == self.n_workers:\n",
                        "            self.grads_received = 0\n",
                        "\n",
                        "        return loss\n",
                        "\n",
                        "\n",
                        "----------------------------\n",
                        "File: models.py\n",
                        "\n",
                        "from torch import nn\n",
                        "import torch.nn.functional as F\n",
                        "\n",
                        "import math\n",
                        "\n",
                        "\n",
                        "# MNIST Fully Connected Net ----------------------------------------------------\n",
                        "class MNISTNet(nn.Module):\n",
                        "    def __init__(self):\n",
                        "        super(MNISTNet, self).__init__()\n",
                        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
                        "        self.fc2 = nn.Linear(512, 10)\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        # flatten image input\n",
                        "        x = x.view(-1, 28 * 28)\n",
                        "        x = F.relu(self.fc1(x))\n",
                        "        x = self.fc2(x)\n",
                        "        return x\n",
                        "\n",
                        "\n",
                        "class MNISTLogReg(nn.Module):\n",
                        "    def __init__(self):\n",
                        "        super(MNISTLogReg, self).__init__()\n",
                        "        self.fc = nn.Linear(28 * 28, 10)\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        # flatten image input\n",
                        "        x = x.view(-1, 28 * 28)\n",
                        "        x = self.fc(x)\n",
                        "        return x\n",
                        "\n",
                        "class BasicBlock(nn.Module):\n",
                        "    expansion = 1\n",
                        "\n",
                        "    def __init__(self, in_planes, planes, stride=1):\n",
                        "        super(BasicBlock, self).__init__()\n",
                        "        self.conv1 = nn.Conv2d(\n",
                        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
                        "        self.bn1 = nn.BatchNorm2d(planes)\n",
                        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
                        "                               stride=1, padding=1, bias=False)\n",
                        "        self.bn2 = nn.BatchNorm2d(planes)\n",
                        "\n",
                        "        self.shortcut = nn.Sequential()\n",
                        "        if stride != 1 or in_planes != self.expansion*planes:\n",
                        "            self.shortcut = nn.Sequential(\n",
                        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
                        "                          kernel_size=1, stride=stride, bias=False),\n",
                        "                nn.BatchNorm2d(self.expansion*planes)\n",
                        "            )\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        out = F.relu(self.bn1(self.conv1(x)))\n",
                        "        out = self.bn2(self.conv2(out))\n",
                        "        out += self.shortcut(x)\n",
                        "        out = F.relu(out)\n",
                        "        return out\n",
                        "\n",
                        "\n",
                        "class Bottleneck(nn.Module):\n",
                        "    expansion = 4\n",
                        "\n",
                        "    def __init__(self, in_planes, planes, stride=1):\n",
                        "        super(Bottleneck, self).__init__()\n",
                        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
                        "        self.bn1 = nn.BatchNorm2d(planes)\n",
                        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
                        "                               stride=stride, padding=1, bias=False)\n",
                        "        self.bn2 = nn.BatchNorm2d(planes)\n",
                        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
                        "                               planes, kernel_size=1, bias=False)\n",
                        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
                        "\n",
                        "        self.shortcut = nn.Sequential()\n",
                        "        if stride != 1 or in_planes != self.expansion*planes:\n",
                        "            self.shortcut = nn.Sequential(\n",
                        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
                        "                          kernel_size=1, stride=stride, bias=False),\n",
                        "                nn.BatchNorm2d(self.expansion*planes)\n",
                        "            )\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        out = F.relu(self.bn1(self.conv1(x)))\n",
                        "        out = F.relu(self.bn2(self.conv2(out)))\n",
                        "        out = self.bn3(self.conv3(out))\n",
                        "        out += self.shortcut(x)\n",
                        "        out = F.relu(out)\n",
                        "        return out\n",
                        "\n",
                        "\n",
                        "class ResNet(nn.Module):\n",
                        "    def __init__(self, block, num_blocks, num_classes=10):\n",
                        "        super(ResNet, self).__init__()\n",
                        "        self.in_planes = 64\n",
                        "\n",
                        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
                        "                               stride=1, padding=1, bias=False)\n",
                        "        self.bn1 = nn.BatchNorm2d(64)\n",
                        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
                        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
                        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
                        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
                        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
                        "\n",
                        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
                        "        strides = [stride] + [1]*(num_blocks-1)\n",
                        "        layers = []\n",
                        "        for stride in strides:\n",
                        "            layers.append(block(self.in_planes, planes, stride))\n",
                        "            self.in_planes = planes * block.expansion\n",
                        "        return nn.Sequential(*layers)\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        out = F.relu(self.bn1(self.conv1(x)))\n",
                        "        out = self.layer1(out)\n",
                        "        out = self.layer2(out)\n",
                        "        out = self.layer3(out)\n",
                        "        out = self.layer4(out)\n",
                        "        out = F.avg_pool2d(out, 4)\n",
                        "        out = out.view(out.size(0), -1)\n",
                        "        out = self.linear(out)\n",
                        "        return out\n",
                        "\n",
                        "\n",
                        "def resnet18():\n",
                        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
                        "\n",
                        "\n",
                        "def resnet34():\n",
                        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
                        "\n",
                        "\n",
                        "def resnet50():\n",
                        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
                        "\n",
                        "\n",
                        "def resnet101():\n",
                        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
                        "\n",
                        "\n",
                        "def resnet152():\n",
                        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
                        "\n",
                        "\n",
                        "__all__ = [\n",
                        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
                        "    'vgg19_bn', 'vgg19',\n",
                        "]\n",
                        "\n",
                        "\n",
                        "class VGG(nn.Module):\n",
                        "    \"\"\"\n",
                        "    VGG model\n",
                        "    \"\"\"\n",
                        "    def __init__(self, features):\n",
                        "        super(VGG, self).__init__()\n",
                        "        self.features = features\n",
                        "        self.classifier = nn.Sequential(\n",
                        "            nn.Dropout(),\n",
                        "            nn.Linear(512, 512),\n",
                        "            nn.ReLU(True),\n",
                        "            nn.Dropout(),\n",
                        "            nn.Linear(512, 512),\n",
                        "            nn.ReLU(True),\n",
                        "            nn.Linear(512, 10),\n",
                        "        )\n",
                        "        # Initialize weights\n",
                        "        for m in self.modules():\n",
                        "            if isinstance(m, nn.Conv2d):\n",
                        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
                        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
                        "                m.bias.data.zero_()\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        x = self.features(x)\n",
                        "        x = x.view(x.size(0), -1)\n",
                        "        x = self.classifier(x)\n",
                        "        return x\n",
                        "\n",
                        "\n",
                        "def make_layers(config, batch_norm=False):\n",
                        "    layers = []\n",
                        "    in_channels = 3\n",
                        "    for v in config:\n",
                        "        if v == 'M':\n",
                        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
                        "        else:\n",
                        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
                        "            if batch_norm:\n",
                        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
                        "            else:\n",
                        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
                        "            in_channels = v\n",
                        "    return nn.Sequential(*layers)\n",
                        "\n",
                        "\n",
                        "cfg = {\n",
                        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
                        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
                        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
                        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M',\n",
                        "          512, 512, 512, 512, 'M'],\n",
                        "}\n",
                        "\n",
                        "\n",
                        "def vgg11():\n",
                        "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
                        "    return VGG(make_layers(cfg['A']))\n",
                        "\n",
                        "\n",
                        "def vgg11_bn():\n",
                        "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
                        "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
                        "\n",
                        "\n",
                        "def vgg13():\n",
                        "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
                        "    return VGG(make_layers(cfg['B']))\n",
                        "\n",
                        "\n",
                        "def vgg13_bn():\n",
                        "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
                        "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
                        "\n",
                        "\n",
                        "def vgg16():\n",
                        "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
                        "    return VGG(make_layers(cfg['D']))\n",
                        "\n",
                        "\n",
                        "def vgg16_bn():\n",
                        "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
                        "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
                        "\n",
                        "\n",
                        "def vgg19():\n",
                        "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
                        "    return VGG(make_layers(cfg['E']))\n",
                        "\n",
                        "\n",
                        "def vgg19_bn():\n",
                        "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
                        "    return VGG(make_layers(cfg['E'], batch_norm=True))\n",
                        "\n",
                        "\n",
                        "----------------------------\n",
                        "File: prep_data.py\n",
                        "\n",
                        "import numpy as np\n",
                        "from torchvision import datasets\n",
                        "import torchvision.transforms as transforms\n",
                        "from torch.utils.data import DataLoader, Subset\n",
                        "\n",
                        "\n",
                        "def create_loaders(dataset_name, n_workers, batch_size, seed=42):\n",
                        "\n",
                        "    train_data, test_data = load_data(dataset_name)\n",
                        "\n",
                        "    train_loader_workers = dict()\n",
                        "    n = len(train_data)\n",
                        "\n",
                        "    # preparing iterators for workers and validation set\n",
                        "    np.random.seed(seed)\n",
                        "    indices = np.arange(n)\n",
                        "    np.random.shuffle(indices)\n",
                        "\n",
                        "    n_val = int(np.floor(0.1 * n))\n",
                        "    val_data = Subset(train_data, indices=indices[:n_val])\n",
                        "\n",
                        "    indices = indices[n_val:]\n",
                        "    n = len(indices)\n",
                        "    a = int(np.floor(n / (2*n_workers)))\n",
                        "    top_ind = a * n_workers\n",
                        "    #top_ind = 0\n",
                        "    seq = range(a, top_ind, a)\n",
                        "    split = np.split(indices[:top_ind], seq)\n",
                        "    #split = [np.array([]).astype(int) for _ in range(n_workers)]\n",
                        "    \n",
                        "    \n",
                        "    for idx in indices[top_ind:]:\n",
                        "    #for idx in indices:\n",
                        "        cur_cl = train_data[idx][1]\n",
                        "        split[cur_cl] = np.append(split[cur_cl], idx)\n",
                        "        \n",
                        "    min_len = min([len(split[i]) for i in range(n_workers)])\n",
                        "    \n",
                        "    for idx in range(n_workers):\n",
                        "        split[idx] = split[idx][:min_len]\n",
                        "\n",
                        "\n",
                        "    b = 0\n",
                        "    for ind in split:\n",
                        "        train_loader_workers[b] = DataLoader(Subset(train_data, ind), batch_size=batch_size, shuffle=True)\n",
                        "        b = b + 1\n",
                        "\n",
                        "    test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
                        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
                        "\n",
                        "    return train_loader_workers, val_loader, test_loader\n",
                        "\n",
                        "\n",
                        "def load_data(dataset_name):\n",
                        "\n",
                        "    if dataset_name == 'mnist':\n",
                        "\n",
                        "        transform = transforms.ToTensor()\n",
                        "\n",
                        "        train_data = datasets.MNIST(root='data', train=True,\n",
                        "                                    download=True, transform=transform)\n",
                        "\n",
                        "        test_data = datasets.MNIST(root='data', train=False,\n",
                        "                                   download=True, transform=transform)\n",
                        "    elif dataset_name == 'cifar10':\n",
                        "\n",
                        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
                        "                                         std=[0.229, 0.224, 0.225])\n",
                        "        transform = transforms.Compose([\n",
                        "            transforms.ToTensor(),\n",
                        "            normalize,\n",
                        "        ])\n",
                        "\n",
                        "        train_data = datasets.CIFAR10(root='data', train=True,\n",
                        "                                      download=True, transform=transform)\n",
                        "\n",
                        "        test_data = datasets.CIFAR10(root='data', train=False,\n",
                        "                                     download=True, transform=transform)\n",
                        "    elif dataset_name == 'cifar100':\n",
                        "        transform = transforms.ToTensor()  # add extra transforms\n",
                        "        train_data = datasets.CIFAR100(root='data', train=True,\n",
                        "                                       download=True, transform=transform)\n",
                        "\n",
                        "        test_data = datasets.CIFAR100(root='data', train=False,\n",
                        "                                      download=True, transform=transform)\n",
                        "    else:\n",
                        "        raise ValueError(dataset_name + ' is not known.')\n",
                        "\n",
                        "    return train_data, test_data\n",
                        "\n",
                        "\n",
                        "----------------------------\n",
                        "File: quant.py\n",
                        "\n",
                        "import torch\n",
                        "import numpy as np\n",
                        "\n",
                        "\n",
                        "def prep_grad(x):\n",
                        "    x_flat = torch.unsqueeze(x, 0).flatten()\n",
                        "    dim = x.shape\n",
                        "    d = x_flat.shape[0]\n",
                        "    return x_flat, dim, d\n",
                        "\n",
                        "\n",
                        "def c_nat(x):\n",
                        "    x, dim, d = prep_grad(x)\n",
                        "    # get 2^n out of input\n",
                        "    h1 = torch.floor(torch.where(x != 0, torch.log2(torch.abs(x)), x))\n",
                        "    h2 = torch.where(x != 0, torch.pow(2, h1), x)\n",
                        "    # extract probability\n",
                        "    p = torch.where(x != 0, torch.div(torch.abs(x) - h2, h2), x)\n",
                        "    # sample random uniform vector\n",
                        "    unif = torch.rand_like(x)\n",
                        "    # generate zero one with probability p\n",
                        "    zero_one = torch.floor(unif + p)\n",
                        "    # generate output\n",
                        "    nat = torch.sign(x) * h2 * (1 + zero_one)\n",
                        "    return nat.reshape(dim)\n",
                        "\n",
                        "\n",
                        "def random_dithering_opt(x, p, s, natural):\n",
                        "    \"\"\"\n",
                        "    :param x: vector to quantize\n",
                        "    :param p: norm parameter\n",
                        "    :param s: number of levels\n",
                        "    :param natural: if True, natural dithering is used\n",
                        "    :return: compressed vector\n",
                        "    \"\"\"\n",
                        "    x, dim, d = prep_grad(x)\n",
                        "    # definition of random dithering\n",
                        "    norm = torch.norm(x, p=p)\n",
                        "    if norm == 0:\n",
                        "        return x.reshape(dim)\n",
                        "\n",
                        "    if natural:\n",
                        "        s = int(2 ** (s - 1))\n",
                        "    f = torch.floor(s * torch.abs(x) / norm + torch.rand_like(x))/s\n",
                        "\n",
                        "    if natural:\n",
                        "        f = c_nat(f)\n",
                        "    res = torch.sign(x) * f\n",
                        "    k = res * norm\n",
                        "    return k.reshape(dim)\n",
                        "\n",
                        "\n",
                        "def random_dithering_wrap(p=np.inf, s=2, natural=True):\n",
                        "    def random_dithering(x):\n",
                        "        return random_dithering_opt(x, p=p, s=s, natural=natural)\n",
                        "    return random_dithering\n",
                        "\n",
                        "\n",
                        "def rand_spars_opt(x, h):\n",
                        "    \"\"\"\n",
                        "    :param x: vector to sparsify\n",
                        "    :param h: density\n",
                        "    :return: compressed vector\n",
                        "    \"\"\"\n",
                        "    x, dim, d = prep_grad(x)\n",
                        "    # number of coordinates to keep\n",
                        "    r = int(np.maximum(1, np.floor(d * h)))\n",
                        "    # random vector of r ones and d-r zeros\n",
                        "    mask = torch.zeros_like(x)\n",
                        "    mask[torch.randperm(d)[:r]] = 1\n",
                        "    # just r random coordinates are kept\n",
                        "    t = mask * x * (d/r)\n",
                        "    t = t.reshape(dim)\n",
                        "    return t\n",
                        "\n",
                        "\n",
                        "def rand_spars_wrap(h=0.1):\n",
                        "    def rand_spars(x):\n",
                        "        return rand_spars_opt(x, h=h)\n",
                        "    return rand_spars\n",
                        "\n",
                        "\n",
                        "def top_k_opt(x, h):\n",
                        "    \"\"\"\n",
                        "    :param x: vector to sparsify\n",
                        "    :param h: density\n",
                        "    :return: compressed vector\n",
                        "    \"\"\"\n",
                        "    x, dim, d = prep_grad(x)\n",
                        "    # number of coordinates kept\n",
                        "    r = int(np.maximum(1, np.floor(d * h)))\n",
                        "    # positions of top_k coordinates\n",
                        "    _, ind = torch.topk(torch.abs(x), r)\n",
                        "    mask = torch.zeros_like(x)\n",
                        "    mask[ind] = 1\n",
                        "    t = mask * x\n",
                        "    t = t.reshape(dim)\n",
                        "    return t\n",
                        "\n",
                        "\n",
                        "def top_k_wrap(h=0.1):\n",
                        "    def top_k(x):\n",
                        "        return top_k_opt(x, h=h)\n",
                        "    return top_k\n",
                        "    \n",
                        "\n",
                        "def grad_spars_opt(x, h, max_it):\n",
                        "    \"\"\"\n",
                        "    :param x: vector to sparsify\n",
                        "    :param h: density\n",
                        "    :param max_it: maximum number of iterations of greedy algorithm\n",
                        "    :return: compressed vector\n",
                        "    \"\"\"\n",
                        "    x, dim, d = prep_grad(x)\n",
                        "    # number of coordinates kept\n",
                        "    r = int(np.maximum(1, np.floor(d * h)))\n",
                        "\n",
                        "    abs_x = torch.abs(x)\n",
                        "    abs_sum = torch.sum(abs_x)\n",
                        "    ones = torch.ones_like(x)\n",
                        "    p_0 = r * abs_x / abs_sum\n",
                        "    p = torch.min(p_0, ones)\n",
                        "    for _ in range(max_it):\n",
                        "        p_sub = p[(p != 1).nonzero(as_tuple=True)]\n",
                        "        p = torch.where(p >= ones, ones, p)\n",
                        "        if len(p_sub) == 0 or torch.sum(torch.abs(p_sub)) == 0:\n",
                        "            break\n",
                        "        else:\n",
                        "            c = (r - d + len(p_sub))/torch.sum(p_sub)\n",
                        "        p = torch.min(c * p, ones)\n",
                        "        if c <= 1:\n",
                        "            break\n",
                        "    prob = torch.rand_like(x)\n",
                        "    # avoid making very small gradient too big\n",
                        "    s = torch.where(p <= 10**(-6), x, x / p)\n",
                        "    # we keep just coordinates with high probability\n",
                        "    t = torch.where(prob <= p, s, torch.zeros_like(x))\n",
                        "    t = t.reshape(dim)\n",
                        "    return t\n",
                        "\n",
                        "\n",
                        "def grad_spars_wrap(h=0.1, max_it=4):\n",
                        "    def grad_spars(x):\n",
                        "        return grad_spars_opt(x, h=h, max_it=max_it)\n",
                        "    return grad_spars\n",
                        "\n",
                        "\n",
                        "def biased_unbiased_wrap(biased_comp, unbiased_comp):\n",
                        "    def error_quant(x):\n",
                        "        c_1 = biased_comp(x)\n",
                        "        error = x - c_1\n",
                        "        c_2 = unbiased_comp(error)\n",
                        "        return c_1 + c_2\n",
                        "    return error_quant\n",
                        "\n",
                        "\n",
                        "def combine_two_wrap(comp_1, comp_2):\n",
                        "    def combine(x):\n",
                        "        t_1 = comp_1(x)\n",
                        "        t_2 = comp_2(t_1)\n",
                        "        return t_2\n",
                        "    return combine\n",
                        "\n",
                        "\n",
                        "----------------------------\n",
                        "File: run_training_EF21_HM_NORM_20250923_180935.py\n",
                        "\n",
                        "import os, gc, torch, wandb\n",
                        "import numpy as np\n",
                        "from torch.nn import CrossEntropyLoss\n",
                        "from quant import top_k_wrap\n",
                        "from utils import create_exp, myrepr\n",
                        "from train import tune_step_size\n",
                        "from models import resnet18\n",
                        "from prep_data import create_loaders\n",
                        "# ---------------------------------------------------------------------------\n",
                        "\n",
                        "# ------------------------- USER CONFIG -------------------------------------\n",
                        "n_workers   = 10\n",
                        "bs          = 64\n",
                        "h           = 0.1\n",
                        "project_name= \"EF21_SOM\"\n",
                        "\n",
                        "gen_dict = {\"model_architecture\": resnet18, \"model_architecture_str\": \"resnet18\", \"dataset\": \"cifar10\",\n",
                        "            \"epochs\": 90, \"ef_methods\": [os.environ[\"EF_METHOD\"]]}\n",
                        "\n",
                        "presets = {\n",
                        "    \"EF21\":          {\"lrs\":[1.0], \"etas\":[None], \"p_exps\":[None], \"q_exps\":[None], \"cuda_device\":\"cuda:1\"},\n",
                        "    \"ECONTROL\":      {\"lrs\":[1.0], \"etas\":[0.1],  \"p_exps\":[None], \"q_exps\":[None], \"cuda_device\":\"cuda:1\"},\n",
                        "    \"EF21_SGDM\":     {\"lrs\":[0.1], \"etas\":[0.1], \"p_exps\":[None], \"q_exps\":[None], \"cuda_device\":\"cuda:1\"},\n",
                        "    \n",
                        "    \"EF21_SGDM_NORM\":    {\"lrs\":[0.1], \"etas\":[1.0], \"p_exps\":[None], \"q_exps\":[0.5], \"cuda_device\":\"cuda:1\"},\n",
                        "    \"EF21_HM_NORM\": {\"lrs\":[0.1], \"etas\":[1.0], \"p_exps\":[None], \"q_exps\":[0.67], \"cuda_device\":\"cuda:1\"},\n",
                        "    \"EF21_RHM_NORM\":          {\"lrs\":[0.1], \"etas\":[1.0], \"p_exps\":[None], \"q_exps\":[0.67], \"cuda_device\":\"cuda:1\"},\n",
                        "    \"EF21_MVR_NORM\":       {\"lrs\":[0.1], \"etas\":[1.0], \"p_exps\":[None], \"q_exps\":[0.67], \"cuda_device\":\"cuda:1\"},\n",
                        "    \"EF21_IGT_NORM\":          {\"lrs\":[0.1], \"etas\":[1.0], \"p_exps\":[None], \"q_exps\":[0.57], \"cuda_device\":\"cuda:1\"}   \n",
                        "}\n",
                        "\n",
                        "\n",
                        "# ---------------------------------------------------------------------------\n",
                        "\n",
                        "model_architecture = gen_dict[\"model_architecture\"]\n",
                        "dataset            = gen_dict[\"dataset\"]\n",
                        "epochs             = gen_dict[\"epochs\"]\n",
                        "model_architecture_str = gen_dict[\"model_architecture_str\"] \n",
                        "\n",
                        "for ef_method in gen_dict[\"ef_methods\"]:\n",
                        "    exp_dict = gen_dict|presets[ef_method] # merge the dictionaries\n",
                        "    cuda_device      = exp_dict[\"cuda_device\"]\n",
                        "    print(f\"Running experiment with {ef_method}\")\n",
                        "    for lr in exp_dict[\"lrs\"]:\n",
                        "        for eta in exp_dict[\"etas\"]:\n",
                        "            for p_exp in exp_dict[\"p_exps\"]:\n",
                        "                exp_dict[\"lr_schedule\"] = \"poly\" if p_exp is not None else None           \n",
                        "                for q_exp in exp_dict[\"q_exps\"]:\n",
                        "                    exp_dict[\"eta_schedule\"] = \"poly\" if q_exp is not None else None\n",
                        "                    exp_name = (f\"{model_architecture.__name__}_{dataset}_{ef_method}\"\n",
                        "                                f\"_topk-{myrepr(h)}\"f\"_lr-{myrepr(lr)}\"f\"_eta-{myrepr(eta)}\"f\"_p-{myrepr(p_exp)}\"f\"_q-{myrepr(q_exp)}\")\n",
                        "                    \n",
                        "                    exp = create_exp(name=exp_name, dataset=dataset, net=model_architecture, n_workers=n_workers, epochs=epochs, seed=42,\n",
                        "                                    batch_size=bs, lrs=[lr], etas=[eta], lr_schedule=exp_dict[\"lr_schedule\"], eta_schedule=exp_dict[\"eta_schedule\"],\n",
                        "                                    compression={'wrapper': False, 'compression': top_k_wrap(h=h)}, error_feedback=ef_method,\n",
                        "                                    criterion=CrossEntropyLoss(), master_compression=None, momentum=0, weight_decay=0, p=p_exp, q=q_exp)\n",
                        "\n",
                        "                    wandb.init(project=project_name, name=f\"{exp_name}_{cuda_device}\", config={**exp, \"lr\": lr, \"eta\": eta, \"cuda_device\": cuda_device}, \n",
                        "                            tags=[exp['error_feedback'], model_architecture_str, dataset])\n",
                        "\n",
                        "                    best_lr, best_acc_lr = tune_step_size(exp, suffix=exp_name, schedule=exp_dict[\"lr_schedule\"], device=cuda_device)\n",
                        "                    wandb.finish()\n",
                        "                    torch.cuda.empty_cache()\n",
                        "                    gc.collect()  # tidy up\n",
                        "                    \n",
                        "                    \n",
                        "if __name__ == \"__main__\":\n",
                        "    pass\n",
                        "\n",
                        "\n",
                        "----------------------------\n",
                        "File: train.py\n",
                        "\n",
                        "import torch\n",
                        "import numpy as np\n",
                        "import time\n",
                        "\n",
                        "from utils import create_run, update_run, save_run, seed_everything, log_series\n",
                        "from prep_data import create_loaders\n",
                        "from gen_sgd import SGDGen\n",
                        "import wandb\n",
                        "\n",
                        "\n",
                        "RUNS = 3\n",
                        "#torch.set_default_dtype(torch.float64)\n",
                        "\n",
                        "# def train_workers(suffix, model, optimizer, criterion, epochs, train_loader_workers,\n",
                        "#                   val_loader, test_loader, n_workers, hpo=False, scheduler=None):\n",
                        "\n",
                        "def train_workers(suffix, model, optimizer, criterion, epochs, train_loader_workers,\n",
                        "                   val_loader, test_loader, n_workers, device=None, hpo=False, lr_scheduler=None, eta_schedule=None, q_exp=None):\n",
                        "    \n",
                        "    ##################################################\n",
                        "    #new \n",
                        "    run = create_run()\n",
                        "\n",
                        "    best_val_loss = np.inf\n",
                        "    best_val_acc  = 0.0\n",
                        "    test_loss = np.inf\n",
                        "    test_acc  = 0.0\n",
                        "\n",
                        "    cum_bp_eq = 0.0\n",
                        "    cum_ex_bp = 0.0\n",
                        "\n",
                        "    start_time = torch.cuda.Event(enable_timing=True) if (device and \"cuda\" in str(device)) else None\n",
                        "    end_time   = torch.cuda.Event(enable_timing=True) if (device and \"cuda\" in str(device)) else None\n",
                        "\n",
                        "    # =========================\n",
                        "    # BASELINE SNAPSHOT (epoch 0)\n",
                        "    # =========================\n",
                        "    # CHANGED: compute both val & test at initial weights first\n",
                        "    val_loss, val_acc   = accuracy_and_loss(model, val_loader,  criterion, device)\n",
                        "    test_loss, test_acc = accuracy_and_loss(model, test_loader, criterion, device)\n",
                        "    best_val_loss = val_loss\n",
                        "    best_val_acc  = val_acc\n",
                        "\n",
                        "    model.eval()\n",
                        "    with torch.no_grad():\n",
                        "        total_loss = 0.0\n",
                        "        n_batches  = 0\n",
                        "        for w in range(n_workers):\n",
                        "            for data, labels in train_loader_workers[w]:\n",
                        "                data, labels = data.to(device), labels.to(device)\n",
                        "                out   = model(data)\n",
                        "                total_loss += criterion(out, labels).item()\n",
                        "                n_batches  += 1\n",
                        "    train_loss0 = total_loss / max(1, n_batches)\n",
                        "\n",
                        "    update_run(train_loss0, test_loss, test_acc, run)\n",
                        "    log_series(\n",
                        "        run,\n",
                        "        epoch=0,\n",
                        "        lr=optimizer.param_groups[0][\"lr\"],\n",
                        "        eta=optimizer.param_groups[0][\"eta\"],\n",
                        "        val_loss=val_loss,\n",
                        "        val_acc=val_acc,\n",
                        "        cum_bp_eq_total=0.0,\n",
                        "        cum_ex_bp_total=0.0,\n",
                        "        wall_seconds=0.0,\n",
                        "        gpu_seconds=None,\n",
                        "    )\n",
                        "\n",
                        "    wandb.log({\n",
                        "        \"epoch\": 0,\n",
                        "        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
                        "        \"eta\": optimizer.param_groups[0][\"eta\"],\n",
                        "        \"train_loss\": train_loss0,\n",
                        "        \"val_loss\": val_loss,\n",
                        "        \"test_loss\": test_loss,\n",
                        "        \"val_acc\": val_acc,\n",
                        "        \"test_acc\": test_acc,\n",
                        "        \"cum_bp_eq\": 0.0,\n",
                        "        \"cum_ex_bp\": 0.0,\n",
                        "        \"wall_seconds\": 0.0,\n",
                        "    })\n",
                        "    \n",
                        "    #model.train()                  # (put this back right after the snapshot)\n",
                        "    wall_start = time.time()       # <<< reset so epoch 1 wall_seconds starts at ~0\n",
                        "\n",
                        "\n",
                        "    \n",
                        "\n",
                        "    \n",
                        "    # =========================\n",
                        "    #        EPOCH LOOP\n",
                        "    # =========================\n",
                        "    for e in range(epochs):\n",
                        "        model.train()\n",
                        "        \n",
                        "        # start per-epoch GPU timing (if CUDA)\n",
                        "        if start_time is not None:\n",
                        "            start_time.record()\n",
                        "        \n",
                        "        running_loss = 0\n",
                        "        train_loader_iter = [iter(train_loader_workers[w]) for w in range(n_workers)]\n",
                        "        iter_steps = len(train_loader_workers[0])\n",
                        "        for _ in range(iter_steps):\n",
                        "            for w_id in range(n_workers):\n",
                        "                \n",
                        "                data, labels = next(train_loader_iter[w_id])\n",
                        "                data, labels = data.to(device), labels.to(device)\n",
                        "\n",
                        "                optimizer.zero_grad(set_to_none=True)\n",
                        "                bp_eq = 1                                        # default 1 backprop\n",
                        "                ef = optimizer.error_feedback\n",
                        "                eta_now = optimizer.param_groups[0]['eta']\n",
                        "\n",
                        "                loss_item = None\n",
                        "                \n",
                        "                # -------------------- EF21-IGT (single backprop at extrapolated x) --------------------\n",
                        "                if ef == \"EF21_IGT_NORM\":\n",
                        "                    bp_eq = 1\n",
                        "                    # CHANGED: no pre-branch forward; compute only at x_extr\n",
                        "                    disp = []\n",
                        "                    for p in model.parameters():\n",
                        "                        s  = optimizer.state[p]\n",
                        "                        kp = f\"x_prev_{w_id}\"\n",
                        "                        if kp not in s:\n",
                        "                            s[kp] = p.data.detach().clone()\n",
                        "                        disp.append((p.data - s[kp]).detach())\n",
                        "\n",
                        "                    scale = (1 - eta_now) / max(eta_now, 1e-12)\n",
                        "                    with torch.no_grad():\n",
                        "                        saved_params = [p.data.detach().clone() for p in model.parameters()]\n",
                        "                        for p, d in zip(model.parameters(), disp):\n",
                        "                            p.data = (p.data + scale * d).detach()\n",
                        "\n",
                        "                    out_ex = model(data); loss_ex = criterion(out_ex, labels)\n",
                        "                    loss_ex.backward()\n",
                        "                    loss_item = loss_ex.item()                     # CHANGED\n",
                        "\n",
                        "                    with torch.no_grad():\n",
                        "                        for p, sdata in zip(model.parameters(), saved_params):\n",
                        "                            p.data = sdata\n",
                        "\n",
                        "                    optimizer.step_local_global(w_id)\n",
                        "                \n",
                        "                # -------------------- EF21-RHM (3 bp-equiv: 1 grad at x^k + HVP at x_hat) ------------\n",
                        "                elif ef == \"EF21_RHM_NORM\":\n",
                        "                    bp_eq = 3\n",
                        "                    out = model(data); loss = criterion(out, labels)   # CHANGED: compute here\n",
                        "                    loss.backward(create_graph=True)\n",
                        "                    loss_item = loss.item()\n",
                        "\n",
                        "                    disp = []\n",
                        "                    for p in model.parameters():\n",
                        "                        s  = optimizer.state[p]\n",
                        "                        kp = f\"x_prev_{w_id}\"\n",
                        "                        if kp not in s:\n",
                        "                            s[kp] = p.data.detach().clone()\n",
                        "                        disp.append((p.data - s[kp]).detach())\n",
                        "\n",
                        "                    q_t = torch.rand(1).item()\n",
                        "                    with torch.no_grad():\n",
                        "                        saved_params = [p.data.detach().clone() for p in model.parameters()]\n",
                        "                        for p in model.parameters():\n",
                        "                            s  = optimizer.state[p]\n",
                        "                            kp = f\"x_prev_{w_id}\"\n",
                        "                            xk, xkm1 = p.data, s[kp]\n",
                        "                            p.data = (q_t * xk + (1 - q_t) * xkm1).detach()\n",
                        "\n",
                        "                    out_hat = model(data); loss_hat = criterion(out_hat, labels)\n",
                        "                    grads_hat = torch.autograd.grad(loss_hat, list(model.parameters()), create_graph=True)\n",
                        "                    hvp = torch.autograd.grad(\n",
                        "                        outputs=grads_hat,\n",
                        "                        inputs=list(model.parameters()),\n",
                        "                        grad_outputs=disp,\n",
                        "                        retain_graph=False,\n",
                        "                        only_inputs=True\n",
                        "                    )\n",
                        "                    hvp_dict = {p: h for p, h in zip(model.parameters(), hvp)}\n",
                        "\n",
                        "                    with torch.no_grad():\n",
                        "                        for p, sdata in zip(model.parameters(), saved_params):\n",
                        "                            p.data = sdata\n",
                        "\n",
                        "                    optimizer.step_local_global(w_id, hvp_dict=hvp_dict)\n",
                        "\n",
                        "                 # -------------------- EF21-MVR_2b (two grads on same minibatch: x^{k-1}, x^k) --------\n",
                        "                elif ef == \"EF21_MVR_NORM\":\n",
                        "                    bp_eq = 2\n",
                        "                    prev_points = []\n",
                        "                    for p in model.parameters():\n",
                        "                        s  = optimizer.state[p]\n",
                        "                        kp = f\"x_prev_{w_id}\"\n",
                        "                        if kp not in s:\n",
                        "                            s[kp] = p.data.detach().clone()\n",
                        "                        prev_points.append(s[kp])\n",
                        "\n",
                        "                    with torch.no_grad():\n",
                        "                        saved_params = [p.data.detach().clone() for p in model.parameters()]\n",
                        "                        for p, xkm1 in zip(model.parameters(), prev_points):\n",
                        "                            p.data = xkm1.detach()\n",
                        "\n",
                        "                    out_prev = model(data); loss_prev = criterion(out_prev, labels)\n",
                        "                    grads_prev = torch.autograd.grad(loss_prev, list(model.parameters()), create_graph=False)\n",
                        "                    gprev_dict = {p: g for p, g in zip(model.parameters(), grads_prev)}\n",
                        "\n",
                        "                    with torch.no_grad():\n",
                        "                        for p, sdata in zip(model.parameters(), saved_params):\n",
                        "                            p.data = sdata\n",
                        "\n",
                        "                    optimizer.zero_grad(set_to_none=True)\n",
                        "                    out = model(data); loss = criterion(out, labels)\n",
                        "                    loss.backward()\n",
                        "                    loss_item = loss.item()\n",
                        "\n",
                        "                    optimizer.step_local_global(w_id, mvr2b_gprev=gprev_dict)\n",
                        "\n",
                        "                elif ef == \"EF21_HM_NORM\":\n",
                        "                    bp_eq = 2\n",
                        "                    out = model(data); loss = criterion(out, labels)   # CHANGED: compute here\n",
                        "                    loss.backward(create_graph=True)\n",
                        "                    loss_item = loss.item()\n",
                        "\n",
                        "                    disp = []\n",
                        "                    for p in model.parameters():\n",
                        "                        s = optimizer.state[p]\n",
                        "                        key = f\"x_prev_{w_id}\"\n",
                        "                        if key not in s:\n",
                        "                            s[key] = p.data.detach().clone()\n",
                        "                        disp.append((p.data - s[key]).detach())\n",
                        "\n",
                        "                    hvp = torch.autograd.grad(\n",
                        "                        outputs=[p.grad for p in model.parameters()],\n",
                        "                        inputs=list(model.parameters()),\n",
                        "                        grad_outputs=disp,\n",
                        "                        retain_graph=False,\n",
                        "                        only_inputs=True\n",
                        "                    )\n",
                        "                    hvp_dict = {p: h for p, h in zip(model.parameters(), hvp)}\n",
                        "                    optimizer.step_local_global(w_id, hvp_dict=hvp_dict)\n",
                        "                else:\n",
                        "                    out = model(data); loss = criterion(out, labels)   # CHANGED: compute here\n",
                        "                    loss.backward()\n",
                        "                    loss_item = loss.item()\n",
                        "                    optimizer.step_local_global(w_id)\n",
                        "                \n",
                        "                # ---- accounting ----\n",
                        "                bsz = data.size(0)\n",
                        "                cum_bp_eq += bp_eq / n_workers\n",
                        "                cum_ex_bp += bp_eq * bsz / n_workers\n",
                        "                running_loss += float(loss_item)                       # CHANGED: accumulate the right loss\n",
                        "   \n",
                        "                ##################################################\n",
                        "                \n",
                        "                #optimizer.zero_grad()\n",
                        "                \n",
                        "        # ---------------------------------------------------------------------------\n",
                        "        if lr_scheduler is not None:\n",
                        "            lr_scheduler.step()\n",
                        "\n",
                        "        # ---------- ETA SCHEDULER ----------\n",
                        "        if eta_schedule == \"poly\" and q_exp is not None:\n",
                        "            eta_init = optimizer.param_groups[0]['eta_init']\n",
                        "            cur_eta  = eta_init * ((2/(e + 2)) ** q_exp)\n",
                        "            for g in optimizer.param_groups:\n",
                        "                g['eta'] = cur_eta\n",
                        "        else:\n",
                        "            cur_eta = optimizer.param_groups[0]['eta']\n",
                        "        # -----------------------------------\n",
                        "\n",
                        "        train_loss = running_loss/(iter_steps*n_workers)\n",
                        "\n",
                        "        val_loss, val_acc = accuracy_and_loss(model, val_loader, criterion, device)\n",
                        "\n",
                        "        test_loss, test_acc = accuracy_and_loss(model, test_loader, criterion, device)\n",
                        "        best_val_acc = val_acc\n",
                        "        best_val_loss = val_loss\n",
                        "\n",
                        "        # end per-epoch GPU timing\n",
                        "        gpu_ms = None\n",
                        "        if end_time is not None:\n",
                        "            end_time.record()\n",
                        "            torch.cuda.synchronize()\n",
                        "            gpu_ms = start_time.elapsed_time(end_time)  # milliseconds since record()\n",
                        "\n",
                        "        wall_elapsed = time.time() - wall_start         # cumulative wall seconds since training start\n",
                        "\n",
                        "\n",
                        "        update_run(train_loss, test_loss, test_acc, run)\n",
                        "        cur_lr = optimizer.param_groups[0][\"lr\"]\n",
                        "                 \n",
                        "        print('Epoch: {}/{} | LR: {:g} | ETA: {} | Train Loss: {:.5f} | Test Loss: {:.5f} | Test Acc: {:.2f} | BP-Eq: {} | Ex-BP: {}'\n",
                        "      .format(e + 1, epochs, cur_lr, cur_eta, train_loss, test_loss, test_acc, cum_bp_eq, cum_ex_bp))\n",
                        "        \n",
                        "        log_series(\n",
                        "            run,\n",
                        "            epoch=e + 1,\n",
                        "            lr=cur_lr,\n",
                        "            eta=cur_eta,\n",
                        "            val_loss=val_loss,\n",
                        "            val_acc=val_acc,\n",
                        "            cum_bp_eq_total=cum_bp_eq,\n",
                        "            cum_ex_bp_total=cum_ex_bp,\n",
                        "            wall_seconds=wall_elapsed,\n",
                        "            gpu_seconds=(gpu_ms / 1000.0) if gpu_ms is not None else None,\n",
                        "        )\n",
                        "        \n",
                        "        \n",
                        "        log_dict = {\n",
                        "            \"epoch\": e + 1,\n",
                        "            \"lr\": cur_lr,\n",
                        "            \"eta\": cur_eta,\n",
                        "            \"train_loss\": train_loss,\n",
                        "            \"val_loss\": val_loss,\n",
                        "            \"test_loss\": test_loss,\n",
                        "            \"val_acc\": val_acc,\n",
                        "            \"test_acc\": test_acc,\n",
                        "            \"cum_bp_eq\": cum_bp_eq,            # ← primary x-axis\n",
                        "            \"cum_ex_bp\": cum_ex_bp,            # ← batch-aware variant\n",
                        "            \"wall_seconds\": wall_elapsed}\n",
                        "        if gpu_ms is not None:\n",
                        "            log_dict[\"gpu_seconds\"] = gpu_ms / 1000.0\n",
                        "        wandb.log(log_dict)\n",
                        "        \n",
                        "        \n",
                        "    print('')\n",
                        "    if not hpo:\n",
                        "        save_run(suffix, run)\n",
                        "\n",
                        "    return best_val_loss, best_val_acc\n",
                        "\n",
                        "\n",
                        "def accuracy_and_loss(model, loader, criterion, device):\n",
                        "    correct = 0\n",
                        "    total_loss = 0\n",
                        "\n",
                        "    model.eval()\n",
                        "    for data, labels in loader:\n",
                        "        #data, labels = data.to(device, dtype=torch.float64), labels.to(device)\n",
                        "        data, labels = data.to(device), labels.to(device)\n",
                        "        \n",
                        "        output = model(data)\n",
                        "        loss = criterion(output, labels)\n",
                        "        total_loss += loss.item()\n",
                        "\n",
                        "        #preds = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
                        "        _, preds = torch.max(output.data, 1)\n",
                        "        correct += (preds == labels).sum().item()\n",
                        "\n",
                        "    accuracy = 100. * correct / len(loader.dataset)\n",
                        "    total_loss = total_loss / len(loader)\n",
                        "\n",
                        "    return total_loss, accuracy\n",
                        "\n",
                        "\n",
                        "def tune_step_size(exp, suffix=None, schedule=None, device=None):\n",
                        "    best_val_loss = np.inf\n",
                        "    best_lr = 0\n",
                        "    best_val_acc = 0\n",
                        "    best_acc_lr = 0\n",
                        "    \n",
                        "    seed = exp['seed']\n",
                        "    seed_everything(seed)\n",
                        "    \n",
                        "    #torch.use_deterministic_algorithms(True)   # raises if a non-det op is hit\n",
                        "    #torch.backends.cudnn.benchmark = False\n",
                        "    #torch.backends.cudnn.deterministic = True\n",
                        "\n",
                        "    hpo = False\n",
                        "    \n",
                        "    exp['val_losses'] = []\n",
                        "    exp['val_accs'] = []\n",
                        "    for idx, lr in enumerate(exp['lrs']):\n",
                        "        for id, eta in enumerate(exp['etas']):\n",
                        "            #print('Learning rate {:2.4f}:'.format(lr), 'Eta {:2.4f}:'.format(eta))\n",
                        "            \n",
                        "            val_loss, val_acc = run_workers(lr, eta, exp, suffix=suffix, hpo=hpo, schedule=schedule, device=device)\n",
                        "            \n",
                        "            exp['val_losses'].append(val_loss)\n",
                        "            exp['val_accs'].append(val_acc)\n",
                        "            if val_loss < best_val_loss:\n",
                        "                best_lr = lr\n",
                        "                best_val_loss = val_loss\n",
                        "                \n",
                        "            if val_acc > best_val_acc:\n",
                        "                best_acc_lr = lr\n",
                        "                best_val_acc = val_acc\n",
                        "            \n",
                        "    return best_lr, best_acc_lr\n",
                        "\n",
                        "def run_workers(lr, eta, exp, suffix=None, hpo=False, schedule=None, device=None):\n",
                        "    dataset_name = exp['dataset_name']\n",
                        "    n_workers = exp['n_workers']\n",
                        "    batch_size = exp['batch_size']\n",
                        "    epochs = exp['epochs']\n",
                        "    criterion = exp['criterion']\n",
                        "    error_feedback = exp['error_feedback']\n",
                        "    momentum = exp['momentum']\n",
                        "    weight_decay = exp['weight_decay']\n",
                        "    compression = get_compression(**exp['compression'])\n",
                        "    master_compression = exp['master_compression']\n",
                        "\n",
                        "    seed_everything(exp['seed'])    \n",
                        "\n",
                        "    net = exp['net']\n",
                        "    model = net()\n",
                        "    if device is None:                                  # caller did not specify\n",
                        "       device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                        "    else:                                               # caller passed e.g. \"cuda:3\"\n",
                        "       device = torch.device(device)\n",
                        "    print(f\"Using device: {device}\")\n",
                        "    \n",
                        "    #model.to(device).double()\n",
                        "    \n",
                        "    model.to(device) \n",
                        "    train_loader_workers, val_loader, test_loader = create_loaders(dataset_name, n_workers, batch_size, seed=exp['seed'])\n",
                        "\n",
                        "    optimizer = SGDGen(model.parameters(), lr=lr, eta=eta, n_workers=n_workers, error_feedback=error_feedback,\n",
                        "                       comp=compression, momentum=momentum, weight_decay=weight_decay, master_comp=master_compression)\n",
                        "    \n",
                        "    for g in optimizer.param_groups:\n",
                        "        g['eta_init'] = g['eta']\n",
                        "    \n",
                        "    if schedule == \"poly\":\n",
                        "        p = exp[\"p\"]\n",
                        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: (1.0 / ((epoch + 1) ** p)))\n",
                        "    elif schedule is not None:   # keep the old StepLR option\n",
                        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
                        "    else:\n",
                        "        scheduler = None\n",
                        "\n",
                        "    val_loss, val_acc = train_workers(suffix, model, optimizer, criterion, epochs, train_loader_workers,\n",
                        "                                 val_loader, test_loader, n_workers, device=device, hpo=hpo, lr_scheduler=scheduler, eta_schedule=exp[\"eta_schedule\"], q_exp=exp[\"q\"])\n",
                        "                             \n",
                        "    return val_loss, val_acc\n",
                        "\n",
                        "\n",
                        "def run_tuned_exp(exp, runs=RUNS, suffix=None):\n",
                        "    if suffix is None:\n",
                        "        suffix = exp['name']\n",
                        "\n",
                        "    lr = exp['lr']\n",
                        "\n",
                        "    if lr is None:\n",
                        "        raise ValueError(\"Tune step size first\")\n",
                        "\n",
                        "    seed = exp['seed']\n",
                        "    seed_everything(seed)\n",
                        "\n",
                        "    for i in range(runs):\n",
                        "        print('Run {:3d}/{:3d}, Name {}:'.format(i+1, runs, suffix))\n",
                        "        suffix_run = suffix + '_' + str(i+1)\n",
                        "        run_workers(lr, exp, suffix_run)\n",
                        "\n",
                        "\n",
                        "def get_single_compression(wrapper, compression, **kwargs):\n",
                        "    if wrapper:\n",
                        "        return compression(**kwargs)\n",
                        "    else:\n",
                        "        return compression\n",
                        "\n",
                        "\n",
                        "def get_compression(combine=None, **kwargs):\n",
                        "    if combine is None:\n",
                        "        return get_single_compression(**kwargs)\n",
                        "    else:\n",
                        "        compression_1 = get_single_compression(**combine['comp_1'])\n",
                        "        compression_2 = get_single_compression(**combine['comp_2'])\n",
                        "        return combine['func'](compression_1, compression_2)\n",
                        "\n",
                        "\n",
                        "----------------------------\n",
                        "File: utils.py\n",
                        "\n",
                        "import numpy as np\n",
                        "import os\n",
                        "import glob\n",
                        "import random\n",
                        "import torch\n",
                        "from pickle import load, dump\n",
                        "from collections import defaultdict\n",
                        "\n",
                        "SAVED_RUNS_PATH = 'saved_data/'\n",
                        "EXP_PATH = 'exps_setup/'\n",
                        "\n",
                        "int_repr_prec = lambda x, prec: int(x) if x.is_integer() else round(x,prec)\n",
                        "myrepr = lambda x: repr(round(x, 8)).replace('.',',') if isinstance(x, float) else repr(x)\n",
                        "intrepr = lambda x: int(x) if x.is_integer() else round(x,8)\n",
                        "\n",
                        "# def save_run(suffix, run):\n",
                        "#     if not os.path.isdir(SAVED_RUNS_PATH):\n",
                        "#         os.mkdir(SAVED_RUNS_PATH)\n",
                        "\n",
                        "#     file = SAVED_RUNS_PATH + suffix + '.pickle'\n",
                        "#     with open(file, 'wb') as f:\n",
                        "#         dump(run, f)\n",
                        "\n",
                        "def save_run(suffix, run):\n",
                        "    if not os.path.isdir(SAVED_RUNS_PATH):\n",
                        "        os.mkdir(SAVED_RUNS_PATH)\n",
                        "    file = SAVED_RUNS_PATH + suffix + '.pickle'\n",
                        "\n",
                        "    # convert the internal defaultdict logger to a plain dict for pickling\n",
                        "    if \"series\" in run:   # new-style run\n",
                        "        flat = {k: list(v) for k, v in run[\"series\"].items()}\n",
                        "        flat.update(run.get(\"meta\", {}))   # harmless if empty\n",
                        "        run_to_save = flat\n",
                        "    else:                 # old-style run\n",
                        "        run_to_save = run\n",
                        "\n",
                        "    with open(file, 'wb') as f:\n",
                        "        dump(run_to_save, f)\n",
                        "\n",
                        "\n",
                        "def read_all_runs(exp, suffix=None):\n",
                        "    if suffix is None:\n",
                        "        suffix = exp['name']\n",
                        "\n",
                        "    runs = list()\n",
                        "    runs_files = glob.glob(SAVED_RUNS_PATH + suffix + '_' + '[1-9]*.pickle')  # reads at most first ten runs\n",
                        "    for run_file in runs_files:\n",
                        "        runs.append(read_run(run_file))\n",
                        "    return runs\n",
                        "\n",
                        "\n",
                        "def read_run(file):\n",
                        "    with open(file, 'rb') as f:\n",
                        "        run = load(f)\n",
                        "    return run\n",
                        "\n",
                        "\n",
                        "# def create_run():\n",
                        "#     run = {'train_loss': [],\n",
                        "#            'test_loss': [],\n",
                        "#            'test_acc': []\n",
                        "#            }\n",
                        "#     return run\n",
                        "\n",
                        "def create_run():\n",
                        "    \"\"\"\n",
                        "    New-style run object:\n",
                        "      run[\"series\"][key] -> list of values (defaultdict(list))\n",
                        "      run[\"meta\"]        -> free-form metadata (optional)\n",
                        "    \"\"\"\n",
                        "    return {\"series\": defaultdict(list), \"meta\": {}}\n",
                        "\n",
                        "def log_series(run, **kwargs):\n",
                        "    \"\"\"\n",
                        "    Append key=value pairs to run[\"series\"]. Skips keys whose value is None.\n",
                        "    \"\"\"\n",
                        "    series = run[\"series\"]\n",
                        "    for k, v in kwargs.items():\n",
                        "        if v is not None:\n",
                        "            series[k].append(v)\n",
                        "\n",
                        "# def update_run(train_loss, test_loss, test_acc, run):\n",
                        "#     run['train_loss'].append(train_loss)\n",
                        "#     run['test_loss'].append(test_loss)\n",
                        "#     run['test_acc'].append(test_acc)\n",
                        "\n",
                        "# Backward-compat: keep the old helper so existing calls don't break\n",
                        "def update_run(train_loss, test_loss, test_acc, run):\n",
                        "    log_series(run, train_loss=train_loss, test_loss=test_loss, test_acc=test_acc)\n",
                        "\n",
                        "\n",
                        "def save_exp(exp):\n",
                        "    if not os.path.isdir(EXP_PATH):\n",
                        "        os.mkdir(EXP_PATH)\n",
                        "\n",
                        "    file = EXP_PATH + exp['name'] + '.pickle'\n",
                        "    with open(file, 'wb') as f:\n",
                        "        dump(exp, f)\n",
                        "\n",
                        "\n",
                        "def load_exp(exp_name):\n",
                        "    file = EXP_PATH + exp_name + '.pickle'\n",
                        "    with open(file, 'rb') as f:\n",
                        "        exp = load(f)\n",
                        "    return exp\n",
                        "\n",
                        "\n",
                        "def create_exp(\n",
                        "        name,\n",
                        "        dataset,\n",
                        "        net,\n",
                        "        n_workers,\n",
                        "        epochs,\n",
                        "        seed,\n",
                        "        batch_size,\n",
                        "        lrs,                  # list of candidate γ₀ values\n",
                        "        etas,                 # list of candidate η₀ values\n",
                        "        *,\n",
                        "        lr_schedule=None,     # \"poly\" or None\n",
                        "        eta_schedule=None,    # \"poly\" or None\n",
                        "        compression,\n",
                        "        error_feedback,\n",
                        "        criterion,\n",
                        "        cuda_device=\"cuda:0\",\n",
                        "        master_compression=None,\n",
                        "        momentum=0,\n",
                        "        weight_decay=0,\n",
                        "        p=None,               # exponent for LR  decay\n",
                        "        q=None                # exponent for ETA decay\n",
                        "):\n",
                        "    \"\"\"\n",
                        "    Pack all hyper-parameters into a single dict that gets threaded\n",
                        "    through the training pipeline and logged to wandb.\n",
                        "\n",
                        "    Parameters marked with '*' are keyword-only for clarity.\n",
                        "    \"\"\"\n",
                        "\n",
                        "    exp = {\n",
                        "        # ---- bookkeeping ----\n",
                        "        \"name\":            name,\n",
                        "        \"dataset_name\":    dataset,\n",
                        "        \"net\":             net,\n",
                        "        \"seed\":            seed,\n",
                        "\n",
                        "        # ---- data / training ----\n",
                        "        \"n_workers\":       n_workers,\n",
                        "        \"epochs\":          epochs,\n",
                        "        \"batch_size\":      batch_size,\n",
                        "\n",
                        "        # ---- optimisation ----\n",
                        "        \"lrs\":             lrs,\n",
                        "        \"lr\":              None,          # will be filled by tune_step_size\n",
                        "        \"etas\":            etas,\n",
                        "        \"compression\":     compression,\n",
                        "        \"master_compression\": master_compression,\n",
                        "        \"error_feedback\":  error_feedback,\n",
                        "        \"criterion\":       criterion,\n",
                        "        \"momentum\":        momentum,\n",
                        "        \"weight_decay\":    weight_decay,\n",
                        "\n",
                        "        # ---- scheduling ----\n",
                        "        \"lr_schedule\":     lr_schedule,   # \"poly\" or None\n",
                        "        \"eta_schedule\":    eta_schedule,  # \"poly\" or None\n",
                        "        \"p\":               p,             # exponent for LR  decay\n",
                        "        \"q\":               q,             # exponent for ETA decay\n",
                        "\n",
                        "        # ---- hardware ----\n",
                        "        \"cuda_device\":     cuda_device\n",
                        "    }\n",
                        "\n",
                        "    return exp\n",
                        "\n",
                        "\n",
                        "def seed_everything(seed=42):\n",
                        "    \"\"\"\n",
                        "    :param seed:\n",
                        "    :return:\n",
                        "    \"\"\"\n",
                        "    random.seed(seed)\n",
                        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                        "    np.random.seed(seed)\n",
                        "    torch.manual_seed(seed)\n",
                        "    torch.cuda.manual_seed(seed)\n",
                        "    torch.cuda.manual_seed_all(seed)\n",
                        "    # some cudnn methods can be random even after fixing the seed\n",
                        "    # unless you tell it to be deterministic\n",
                        "    torch.backends.cudnn.deterministic = True\n",
                        "\n",
                        "\n",
                        "----------------------------\n",
                        "File: vgg_cifar.py\n",
                        "\n",
                        "\"\"\"\n",
                        "VGG11/13/16/19 in Pytorch.\n",
                        "\"\"\"\n",
                        "import torch.nn as nn\n",
                        "\n",
                        "\n",
                        "cfg = {\n",
                        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
                        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M',\n",
                        "              512, 512, 'M', 512, 512, 'M'],\n",
                        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M',\n",
                        "              512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
                        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256,\n",
                        "              'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
                        "}\n",
                        "\n",
                        "GROUP_NORM_LOOKUP = {\n",
                        "    16: 2,  # -> channels per group: 8\n",
                        "    32: 4,  # -> channels per group: 8\n",
                        "    64: 8,  # -> channels per group: 8\n",
                        "    128: 8,  # -> channels per group: 16\n",
                        "    256: 16,  # -> channels per group: 16\n",
                        "    512: 32,  # -> channels per group: 16\n",
                        "    1024: 32,  # -> channels per group: 32\n",
                        "    2048: 32,  # -> channels per group: 64\n",
                        "}\n",
                        "\n",
                        "\n",
                        "def create_norm_layer(num_channels, batch_norm=True):\n",
                        "    if batch_norm:\n",
                        "        return nn.BatchNorm2d(num_channels)\n",
                        "    return nn.GroupNorm(GROUP_NORM_LOOKUP[num_channels], num_channels)\n",
                        "\n",
                        "\n",
                        "class VGG(nn.Module):\n",
                        "    def __init__(self, vgg_name, num_classes=10, batch_norm=True):\n",
                        "        super(VGG, self).__init__()\n",
                        "        self.features = self._make_layers(cfg[vgg_name], batch_norm)\n",
                        "        self.classifier = nn.Linear(512, num_classes)\n",
                        "\n",
                        "    def forward(self, x):\n",
                        "        out = self.features(x)\n",
                        "        out = out.view(out.size(0), -1)\n",
                        "        out = self.classifier(out)\n",
                        "        return out\n",
                        "\n",
                        "    def _make_layers(self, cfg, batch_norm):\n",
                        "        layers = []\n",
                        "        in_channels = 3\n",
                        "        for x in cfg:\n",
                        "            if x == 'M':\n",
                        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
                        "            else:\n",
                        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
                        "                           create_norm_layer(x, batch_norm),\n",
                        "                           nn.ReLU(inplace=True)]\n",
                        "                in_channels = x\n",
                        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
                        "        return nn.Sequential(*layers)\n",
                        "\n",
                        "\n",
                        "def vgg11(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG11', num_classes=num_classes)\n",
                        "\n",
                        "\n",
                        "def vgg13(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG13', num_classes=num_classes)\n",
                        "\n",
                        "\n",
                        "def vgg16(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG16', num_classes=num_classes)\n",
                        "\n",
                        "\n",
                        "def vgg19(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG19', num_classes=num_classes)\n",
                        "\n",
                        "\n",
                        "def vgg11gn(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG11', num_classes=num_classes, batch_norm=False)\n",
                        "\n",
                        "\n",
                        "def vgg13gn(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG13', num_classes=num_classes, batch_norm=False)\n",
                        "\n",
                        "\n",
                        "def vgg16gn(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG16', num_classes=num_classes, batch_norm=False)\n",
                        "\n",
                        "\n",
                        "def vgg19gn(pretrained=False, num_classes=10):\n",
                        "    return VGG('VGG19', num_classes=num_classes, batch_norm=False)\n"
                    ]
                }
            ],
            "source": [
                "%%bash\n",
                "# First, process all .py files\n",
                "find . -type f -name \"*.py\" | sort | while read -r file; do\n",
                "  echo -e \"\\n----------------------------\"\n",
                "  echo \"File: $(basename \"$file\")\"\n",
                "  echo \"\"\n",
                "  cat \"$file\"\n",
                "  echo \"\"\n",
                "done\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "# Then, process all .ipynb files\n",
                "find . -type f -name \"*.ipynb\" | sort | while read -r file; do\n",
                "  echo -e \"\\n----------------------------\"\n",
                "  echo \"File: $(basename \"$file\")\"\n",
                "  echo \"\"\n",
                "  cat \"$file\"\n",
                "  echo \"\"\n",
                "done"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Archive code"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "pyten",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}